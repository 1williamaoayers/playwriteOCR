#!/usr/bin/env python3
"""
ä»Šæ—¥å¤´æ¡èµ„è®¯çˆ¬è™« - DOMè§£æç‰ˆ v6
æ”¹ç”¨ç›´æ¥è§£æHTMLç»“æ„ï¼Œä¸ä½¿ç”¨OCR
"""

import sys
import re
import json
import time
import os
import urllib.parse
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright


def parse_time(text: str) -> datetime:
    now = datetime.now()
    
    if 'åˆ†é’Ÿå‰' in text:
        m = re.search(r'(\d+)åˆ†é’Ÿå‰', text)
        if m: return now - timedelta(minutes=int(m.group(1)))
    if 'å°æ—¶å‰' in text:
        m = re.search(r'(\d+)å°æ—¶å‰', text)
        if m: return now - timedelta(hours=int(m.group(1)))
    if 'å¤©å‰' in text:
        m = re.search(r'(\d+)å¤©å‰', text)
        if m: return now - timedelta(days=int(m.group(1)))
    if 'æ˜¨å¤©' in text: return now - timedelta(days=1)
    if 'å‰å¤©' in text: return now - timedelta(days=2)
    
    m = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
    if m: return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
    
    m = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
    if m:
        month, day = int(m.group(1)), int(m.group(2))
        year = now.year
        if month > now.month or (month == now.month and day > now.day):
            year = now.year - 1
        return datetime(year, month, day)
    
    return datetime(2000, 1, 1)


def extract_news_from_dom(page) -> list:
    """ä» DOM ç»“æ„ç›´æ¥æå–æ–°é—»"""
    news = []
    
    # å°è¯•å¤šç§å¯èƒ½çš„æ–°é—»å¡ç‰‡é€‰æ‹©å™¨
    card_selectors = [
        'div.result-content',          # æœç´¢ç»“æœå¡ç‰‡
        'div[class*="result"]',
        'div[class*="feed-card"]',
        'div[class*="card"]',
        'article',
    ]
    
    for selector in card_selectors:
        try:
            cards = page.locator(selector).all()
            if len(cards) > 2:
                print(f"    ä½¿ç”¨é€‰æ‹©å™¨: {selector} (æ‰¾åˆ° {len(cards)} ä¸ª)")
                
                for card in cards:
                    try:
                        # æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ a æ ‡ç­¾æˆ– h æ ‡ç­¾ï¼‰
                        title = ""
                        title_selectors = ['a', 'h1', 'h2', 'h3', '[class*="title"]']
                        for ts in title_selectors:
                            try:
                                title_elem = card.locator(ts).first
                                if title_elem.count() > 0:
                                    t = title_elem.inner_text().strip()
                                    if len(t) > 20:  # æ ‡é¢˜è‡³å°‘20å­—
                                        title = t
                                        break
                            except:
                                continue
                        
                        if not title or len(title) < 15:
                            continue
                        
                        # æå–é“¾æ¥
                        url = ""
                        try:
                            link = card.locator('a').first
                            if link.count() > 0:
                                url = link.get_attribute('href') or ""
                                if url and not url.startswith('http'):
                                    if url.startswith('//'):
                                        url = 'https:' + url
                                    else:
                                        url = 'https://www.toutiao.com' + url
                        except:
                            pass
                        
                        # æå–æ¥æº
                        source = ""
                        try:
                            source_elem = card.locator('[class*="source"], [class*="author"], [class*="name"]').first
                            if source_elem.count() > 0:
                                source = source_elem.inner_text().strip()[:50]
                        except:
                            pass
                        
                        # æå–æ—¶é—´
                        time_text = ""
                        time_obj = datetime(2000, 1, 1)
                        try:
                            # æ•´ä¸ªå¡ç‰‡çš„æ–‡æœ¬ä¸­æœç´¢æ—¶é—´
                            card_text = card.inner_text()
                            time_patterns = [
                                r'\d+åˆ†é’Ÿå‰', r'\d+å°æ—¶å‰', r'\d+å¤©å‰',
                                r'æ˜¨å¤©', r'å‰å¤©',
                                r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
                                r'\d{1,2}æœˆ\d{1,2}æ—¥'
                            ]
                            for pattern in time_patterns:
                                match = re.search(pattern, card_text)
                                if match:
                                    time_text = match.group()
                                    time_obj = parse_time(time_text)
                                    break
                        except:
                            pass
                        
                        news.append({
                            'title': title,
                            'url': url,
                            'source': source,
                            'time': time_obj,
                            'time_text': time_text,
                        })
                    except Exception as e:
                        continue
                
                if len(news) > 0:
                    break  # æ‰¾åˆ°äº†å°±ä¸å†å°è¯•å…¶ä»–é€‰æ‹©å™¨
        except:
            continue
    
    return news


def scrape(keyword: str, pages: int = 5) -> list:
    all_news = []
    os.makedirs('screenshots', exist_ok=True)
    
    with sync_playwright() as p:
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=True, args=['--no-sandbox'])
        
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',
        )
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # ç›´æ¥ä½¿ç”¨ URL ç¼–ç è®¿é—®
            encoded_keyword = urllib.parse.quote(keyword)
            url = f'https://so.toutiao.com/search?dvpf=pc&source=pagination&keyword={encoded_keyword}'
            print(f"ğŸ“„ æ‰“å¼€: {url}")
            page.goto(url, wait_until='domcontentloaded', timeout=60000)
            
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
            time.sleep(6)
            
            print("ğŸ“° ç‚¹å‡»èµ„è®¯...")
            try:
                page.click('text=èµ„è®¯', timeout=5000)
                time.sleep(3)
                print("âœ… å·²ç‚¹å‡»èµ„è®¯")
            except:
                print("âš ï¸ èµ„è®¯æ ‡ç­¾ç‚¹å‡»å¤±è´¥")
            
            for page_num in range(1, pages + 1):
                print(f"\nğŸ“– ç¬¬ {page_num} é¡µ...")
                time.sleep(2)
                
                # æˆªå›¾ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                try:
                    page.screenshot(path=f'screenshots/dom_page_{page_num}.png')
                    print(f"  ğŸ“· æˆªå›¾: dom_page_{page_num}.png")
                except:
                    pass
                
                # DOM è§£ææå–æ–°é—»
                page_news = extract_news_from_dom(page)
                print(f"  ğŸ“° æå–: {len(page_news)} æ¡")
                all_news.extend(page_news)
                
                # ä¿å­˜æå–ç»“æœï¼ˆè°ƒè¯•ç”¨ï¼‰
                with open(f'screenshots/dom_page_{page_num}.json', 'w', encoding='utf-8') as f:
                    json.dump([{'title': n['title'], 'source': n['source'], 'time': n['time_text']} 
                               for n in page_news], f, ensure_ascii=False, indent=2)
                
                if page_num < pages:
                    print(f"  â¡ï¸ ç¿»é¡µ...")
                    try:
                        page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        time.sleep(1)
                        page.click(f'a:text-is("{page_num + 1}")', timeout=5000)
                        time.sleep(3)
                    except:
                        try:
                            page.click('text=ä¸‹ä¸€é¡µ', timeout=3000)
                            time.sleep(3)
                        except:
                            print("  âš ï¸ ç¿»é¡µå¤±è´¥")
                            break
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
        finally:
            browser.close()
            print("\nğŸ”’ æµè§ˆå™¨å…³é—­")
    
    return all_news


def main():
    # è§£æå‚æ•°: keyword [limit] [--json]
    keyword = "å°ç±³é›†å›¢"
    json_mode = False
    
    args = [arg for arg in sys.argv[1:] if arg != '--json']
    json_mode = '--json' in sys.argv
    
    if len(args) >= 1:
        keyword = args[0]
    
    # JSON æ¨¡å¼
    if json_mode:
        import io, sys as sys_module
        old_stdout = sys_module.stdout
        sys_module.stdout = io.StringIO()
        all_news = scrape(keyword)
        sys_module.stdout = old_stdout
        # è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–æ ¼å¼
        output = []
        seen = set()
        for n in all_news:
            key = n['title'][:40]
            if key not in seen:
                seen.add(key)
                output.append({
                    'title': n['title'],
                    'time': n['time'].strftime('%Y-%m-%d %H:%M') if hasattr(n['time'], 'strftime') and n['time'].year > 2000 else '',
                    'url': n.get('url', '')
                })
        output.sort(key=lambda x: x['time'], reverse=True)
        print(json.dumps(output[:20], ensure_ascii=False))
        return
    
    # æ™®é€šæ¨¡å¼
    print("=" * 60)
    print(f"ğŸ¯ ä»Šæ—¥å¤´æ¡çˆ¬è™« v6 (DOMè§£æ) | å…³é”®è¯: {keyword}")
    print("=" * 60)
    
    start = time.time()
    all_news = scrape(keyword)
    elapsed = time.time() - start
    
    # å»é‡
    seen = set()
    unique = []
    for n in all_news:
        key = n['title'][:40]
        if key not in seen:
            seen.add(key)
            unique.append(n)
    
    # æŒ‰æ—¶é—´æ’åº
    unique.sort(key=lambda x: x['time'], reverse=True)
    top20 = unique[:20]
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š åŸå§‹: {len(all_news)} | å»é‡: {len(unique)} | è¾“å‡º: {len(top20)}")
    print("=" * 60)
    
    print(f"\nğŸ“° {keyword} æœ€æ–°èµ„è®¯:\n")
    for i, n in enumerate(top20, 1):
        t = n['time'].strftime('%m-%d') if n['time'].year > 2000 else 'æœªçŸ¥'
        title = n['title'][:65] + '...' if len(n['title']) > 65 else n['title']
        print(f"[{i}] [{t}] {title}")
    
    # ä¿å­˜ MD
    md = f"{keyword}_èµ„è®¯_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(md, 'w', encoding='utf-8') as f:
        f.write(f"# {keyword} æœ€æ–°èµ„è®¯\n\n")
        f.write(f"> é‡‡é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"> æ¥æº: ä»Šæ—¥å¤´æ¡ï¼ˆDOMè§£æï¼Œå‰5é¡µï¼‰\n")
        f.write(f"> å…± {len(top20)} æ¡\n\n---\n\n")
        for i, n in enumerate(top20, 1):
            t = n['time'].strftime('%Y-%m-%d') if n['time'].year > 2000 else 'æœªçŸ¥'
            f.write(f"## {i}. {n['title']}\n\n")
            f.write(f"- **æ—¶é—´**: {t}\n")
            if n.get('source'):
                f.write(f"- **æ¥æº**: {n['source']}\n")
            if n.get('url'):
                f.write(f"- **é“¾æ¥**: [{n['url'][:50]}...]({n['url']})\n")
            f.write(f"\n---\n\n")
    
    print(f"\nğŸ’¾ å·²ä¿å­˜: {md}")
    print(f"â±ï¸ è€—æ—¶: {elapsed:.1f}s")


if __name__ == "__main__":
    main()

