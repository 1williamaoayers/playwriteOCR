#!/usr/bin/env python3
"""
è´¢è”ç¤¾èµ„è®¯çˆ¬è™« - ç®€åŒ–ç‰ˆ
ç”¨æ³•ï¼špython cls_scraper.py "å…³é”®è¯"
åªé‡‡é›†å½“å‰é¡µï¼Œä¸ç¿»é¡µ
"""

import sys
import re
import json
import time
import os
import urllib.parse
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright


def parse_time(text: str) -> datetime:
    """è§£ææ—¶é—´"""
    now = datetime.now()
    
    # "2025-12-31 16:17"
    m = re.search(r'(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2})', text)
    if m:
        return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),
                        int(m.group(4)), int(m.group(5)))
    
    # "01-16 17:40"
    m = re.search(r'(\d{2})-(\d{2})\s+(\d{2}):(\d{2})', text)
    if m:
        month, day = int(m.group(1)), int(m.group(2))
        year = now.year
        if month > now.month:
            year -= 1
        return datetime(year, month, day, int(m.group(3)), int(m.group(4)))
    
    return datetime(2000, 1, 1)


def scrape(keyword: str) -> list:
    """çˆ¬å–è´¢è”ç¤¾ï¼ˆåªé‡‡é›†å½“å‰é¡µï¼‰"""
    news = []
    os.makedirs('screenshots', exist_ok=True)
    
    with sync_playwright() as p:
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=True, args=['--no-sandbox'])
        
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',
        )
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # ç›´æ¥ä½¿ç”¨ URL ç¼–ç è®¿é—®
            encoded_keyword = urllib.parse.quote(keyword)
            url = f'https://www.cls.cn/searchPage?keyword={encoded_keyword}&type=telegram'
            print(f"ğŸ“„ æ‰“å¼€: {url}")
            page.goto(url, wait_until='domcontentloaded', timeout=60000)
            
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
            time.sleep(6)
            
            # æˆªå›¾
            page.screenshot(path='screenshots/cls_page.png')
            print("ğŸ“· æˆªå›¾: cls_page.png")
            
            # ä¿å­˜HTMLç”¨äºè°ƒè¯•
            html = page.content()
            with open('screenshots/cls_page.html', 'w', encoding='utf-8') as f:
                f.write(html)
            print("ğŸ“„ HTML: cls_page.html")
            
            # ç›´æ¥è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬ï¼ŒæŒ‰è¡Œè§£æ
            print("ğŸ“° æå–æ–°é—»...")
            
            # æ–¹æ³•1ï¼šå°è¯•å¸¸è§é€‰æ‹©å™¨
            selectors_to_try = [
                'div.search-telegram-wrap div',
                'div.search-telegram-item',
                'div.telegraph-item',
                'div[class*="telegraph"]',
                'div[class*="telegram"]',
                'div[class*="search"] div',
                'div.content-wrap div',
                'div.list-item',
                'article',
            ]
            
            for selector in selectors_to_try:
                try:
                    items = page.locator(selector).all()
                    if len(items) > 2:
                        print(f"  å°è¯•é€‰æ‹©å™¨: {selector} â†’ {len(items)} ä¸ªå…ƒç´ ")
                        
                        for item in items:
                            try:
                                text = item.inner_text().strip()
                                if len(text) < 30:
                                    continue
                                
                                # è·³è¿‡UIå…ƒç´ 
                                if any(skip in text for skip in ['çƒ­é—¨è¯é¢˜', 'Aè‚¡å…¬å‘Š', 'ç¯çƒå¸‚åœº', '+å…³æ³¨']):
                                    continue
                                
                                # è§£ææ—¶é—´
                                time_obj = parse_time(text)
                                if time_obj.year == 2000:
                                    continue  # æ²¡æœ‰æ—¶é—´çš„è·³è¿‡
                                
                                # æ¸…ç†æ ‡é¢˜
                                title = text
                                # ç§»é™¤æ—¥æœŸæ—¶é—´å‰ç¼€
                                title = re.sub(r'^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}\s*[æ˜ŸæœŸä¸€äºŒä¸‰å››äº”å…­æ—¥]*\s*', '', title)
                                title = title.strip()
                                
                                if len(title) < 20:
                                    continue
                                
                                news.append({
                                    'title': title[:300],
                                    'time': time_obj,
                                })
                            except:
                                continue
                        
                        if len(news) > 0:
                            print(f"  âœ… ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                            break
                except:
                    continue
            
            # æ–¹æ³•2ï¼šå¦‚æœä¸Šé¢æ²¡æå–åˆ°ï¼Œå°è¯•è·å–æ•´ä¸ªé¡µé¢æ–‡æœ¬è§£æ
            if len(news) == 0:
                print("  å°è¯•é¡µé¢æ–‡æœ¬è§£æ...")
                body_text = page.inner_text('body')
                lines = [l.strip() for l in body_text.split('\n') if l.strip()]
                
                for line in lines:
                    if len(line) < 30 or len(line) > 500:
                        continue
                    
                    time_obj = parse_time(line)
                    if time_obj.year == 2000:
                        continue
                    
                    # è·³è¿‡UI
                    if any(skip in line for skip in ['çƒ­é—¨è¯é¢˜', 'Aè‚¡å…¬å‘Š', 'ç¯çƒå¸‚åœº', '+å…³æ³¨', 'åŠ è½½æ›´å¤š']):
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è´¢è”ç¤¾ç‰¹å¾
                    if 'è´¢è”ç¤¾' in line or 'ç”µ' in line:
                        title = re.sub(r'^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}\s*[æ˜ŸæœŸä¸€äºŒä¸‰å››äº”å…­æ—¥]*\s*', '', line)
                        if len(title) > 20:
                            news.append({
                                'title': title[:300],
                                'time': time_obj,
                            })
            
            print(f"ğŸ“Š å…±æå–: {len(news)} æ¡")
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
        finally:
            browser.close()
            print("ğŸ”’ æµè§ˆå™¨å…³é—­")
    
    return news


def main():
    # è§£æå‚æ•°: keyword [limit] [--json]
    keyword = "å°ç±³é›†å›¢"
    json_mode = False
    
    args = [arg for arg in sys.argv[1:] if arg != '--json']
    json_mode = '--json' in sys.argv
    
    if len(args) >= 1:
        keyword = args[0]
    # args[1] æ˜¯ limitï¼Œè´¢è”ç¤¾çˆ¬è™«ä¸ç”¨å®ƒ
    
    # JSON æ¨¡å¼
    if json_mode:
        import io, sys as sys_module
        old_stdout = sys_module.stdout
        sys_module.stdout = io.StringIO()
        all_news = scrape(keyword)
        sys_module.stdout = old_stdout
        # è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–æ ¼å¼
        output = []
        seen = set()
        for n in all_news:
            key = n['title'][:40]
            if key not in seen:
                seen.add(key)
                output.append({
                    'title': n['title'],
                    'time': n['time'].strftime('%Y-%m-%d %H:%M') if hasattr(n['time'], 'strftime') else str(n['time']),
                    'url': ''
                })
        output.sort(key=lambda x: x['time'], reverse=True)
        print(json.dumps(output[:20], ensure_ascii=False))
        return
    
    # æ™®é€šæ¨¡å¼
    print("=" * 60)
    print(f"ğŸ¯ è´¢è”ç¤¾çˆ¬è™« | å…³é”®è¯: {keyword}")
    print("=" * 60)
    
    start = time.time()
    all_news = scrape(keyword)
    elapsed = time.time() - start
    
    # å»é‡
    seen = set()
    unique = []
    for n in all_news:
        key = n['title'][:40]
        if key not in seen:
            seen.add(key)
            unique.append(n)
    
    # æŒ‰æ—¶é—´æ’åº
    unique.sort(key=lambda x: x['time'], reverse=True)
    top20 = unique[:20]
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š åŸå§‹: {len(all_news)} | å»é‡: {len(unique)} | è¾“å‡º: {len(top20)}")
    print("=" * 60)
    
    if top20:
        print(f"\nğŸ“° {keyword} è´¢è”ç¤¾èµ„è®¯:\n")
        for i, n in enumerate(top20, 1):
            t = n['time'].strftime('%m-%d %H:%M')
            title = n['title'][:65] + '...' if len(n['title']) > 65 else n['title']
            print(f"[{i}] [{t}] {title}")
        
        # ä¿å­˜ MD
        md = f"{keyword}_è´¢è”ç¤¾_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(md, 'w', encoding='utf-8') as f:
            f.write(f"# {keyword} è´¢è”ç¤¾èµ„è®¯\n\n")
            f.write(f"> é‡‡é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"> æ¥æº: è´¢è”ç¤¾\n")
            f.write(f"> å…± {len(top20)} æ¡\n\n---\n\n")
            for i, n in enumerate(top20, 1):
                t = n['time'].strftime('%Y-%m-%d %H:%M')
                f.write(f"## {i}. {n['title']}\n\n")
                f.write(f"- **æ—¶é—´**: {t}\n\n---\n\n")
        
        print(f"\nğŸ’¾ å·²ä¿å­˜: {md}")
    else:
        print("\nâš ï¸ æœªæå–åˆ°æ–°é—»ï¼Œè¯·æ£€æŸ¥ screenshots/cls_page.html")
    
    print(f"â±ï¸ è€—æ—¶: {elapsed:.1f}s")


if __name__ == "__main__":
    main()

